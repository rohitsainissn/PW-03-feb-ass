{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "294048e3",
   "metadata": {},
   "source": [
    "Q1. How does bagging reduce overfitting in decision trees?\n",
    "\n",
    "Ans:- As multiple decision trees are used in bagging, hence average of all the output is taken into consideration as a result the overfitting reduced.\n",
    "\n",
    "\n",
    "Q2. What are the advantages and disadvantages of using different types of base learners in bagging?\n",
    "\n",
    "Ans:- Advantages:- overfitting reduced, accuracy increases, more generalised results are produced\n",
    "\n",
    "Disadvantages:- time complexity increases, space complexicity increase.\n",
    "\n",
    "Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?\n",
    "\n",
    "Ans:- If the base learner has high variance (i.e., it is prone to overfitting the training data), bagging can help to reduce the variance of the ensemble model. This is because the averaging of the predictions from multiple models helps to smooth out the noise in the individual predictions and create a more stable, less variable prediction.\n",
    "\n",
    "On the other hand, if the base learner has high bias (i.e., it is underfitting the training data), bagging may not be as effective in reducing bias as other techniques, such as boosting or stacking. This is because averaging multiple biased models may not be enough to correct for the underlying bias in the base learner.\n",
    "Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?\n",
    "\n",
    "Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?\n",
    "\n",
    "Ans:- It is good to have as many ensemble model as possible but larger no of models required high space and time complexity, as a result we have to limit our no of ensemblers depending our model complexity, hence we can do hypertuning to get a point where increasing no. of ensembler is adding no more improvment to model.\n",
    "\n",
    "Q6. Can you provide an example of a real-world application of bagging in machine learning?\n",
    "\n",
    "Ans:- One example of a real-world application of bagging in machine learning is in the field of medical diagnosis. Bagging can be used to build an ensemble model that predicts whether a patient is at risk of developing a certain disease based on multiple medical features such as age, gender, blood pressure, cholesterol levels, and family history.\n",
    "\n",
    "For instance, a bagging ensemble of decision trees can be trained on a dataset of patient medical records. Each decision tree is trained on a bootstrap sample of the data and predicts the risk of disease for a particular patient based on their medical features. The final prediction for a patient is then obtained by averaging the predictions from all the decision trees in the ensemble."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
