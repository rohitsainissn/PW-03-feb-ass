{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dd33b3e",
   "metadata": {},
   "source": [
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?\n",
    "\n",
    "Ans:- Grid search cv is used to sellect the best hyper parameter for a model by calculating its performance with all the combination of hyperparameters and gives the best combination model as output.\n",
    "\n",
    "It takes all the parameters in a dictionary formate and run the model with every possible combination of hyperparameters.\n",
    "\n",
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
    "one over the other?\n",
    "\n",
    "Ans:- In grid search cv we use all the possbile combination of hyperparameters and gives the best set of hyperparameters, while in randomize search cv we randomly search few combination of hyperparameters and gives the best combination out of these,\n",
    "\n",
    "Grid search cv is time consuming because it apply all possible combination while in randomize search cv we can choose how many combination we need to try for sellection the best hyper parameter.\n",
    "\n",
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.\n",
    "\n",
    "Ans:- When some information of the training data set is passed into test data set as a result our test data set performes better, but the model does not perform well in real life or with other data set hence it is a problem need to be solved,\n",
    "\n",
    "One of the main reason of data leakage is when we do featurisation before doing train test split, as a result the test points will also get standardized as per the training data set, and mean, variance information of training data is get transfered to test data set too.\n",
    "\n",
    "\n",
    "Q4. How can you prevent data leakage when building a machine learning model?\n",
    "\n",
    "Ans:- to prevent data leakage we have to split the training and test dataset before doing featurisation like standard scaler.\n",
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?\n",
    "\n",
    "Ans:- Confussion matrix tells the following data in tabular manner shown below: \n",
    "\n",
    "true positive, False Positive\n",
    "\n",
    "Fasle Negative , True Negative\n",
    "\n",
    "True positive are the values positive in reality and predicted positive\n",
    "\n",
    "False positive: predicted positive but negative in reality\n",
    "\n",
    "False negative : predicted negative but positive in reality\n",
    "\n",
    "True Negative: predicted negative and negative in reality\n",
    "\n",
    "\n",
    "Q6. Explain the difference between precision and recall in the context of a confusion matrix.\n",
    "\n",
    "Ans:- Precision = TP/(TP+FP)\n",
    "\n",
    "It is performed to know how much positive predicted values are really positive, the cases in which we have to be very sure about if the positive result i.e. FP must me minimum, we use Precision, like in spam detection, before sending a mail to spam box, we must be sure that the email is a spam.\n",
    "\n",
    "Recall= TP/(TP+FN)\n",
    "\n",
    "Recall is performed to make sure that we have not left any positive case without identifing it. Like in cancer detection test, we have to be sure that no person's report is negative if he have the cancer, i.e. we have to reduce FN.\n",
    "\n",
    "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?\n",
    "\n",
    "Ans:- Confusion matrix is a great way to understand the result of classification. the ideal model must predict all positive value as positive and negative as negative.\n",
    "but model can cause these two types of error:\n",
    "1. Type 1 error :- also known as a false positive, when we call the data as positive but in reality it is negative.\n",
    "\n",
    "2. Type 2 error: False negative, when we call the data as negative but it is positive.\n",
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they\n",
    "calculated?\n",
    "\n",
    "Ans:- We can calculate accuracy = TP+TN  / TP+TN+FP+FN\n",
    "\n",
    "Recall= TP/(TP+FN)\n",
    "\n",
    "Precision = TP/(TP+FP)\n",
    "\n",
    "Specificity: Specificity is the proportion of true negative predictions out of the total number of actual negative cases. It is calculated as TN / (TN + FP).\n",
    "\n",
    "F1 score: F1 score is the harmonic mean of precision and recall, and provides a balanced measure of both metrics. It is calculated as 2 * (precision * recall) / (precision + recall).\n",
    "\n",
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?\n",
    "\n",
    "accuracy = TP+TN  / TP+TN+FP+FN\n",
    "\n",
    "\n",
    "Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning\n",
    "model?\n",
    "\n",
    "Ans:- by calculating accuracy , precision , recall, F1 score we can identify potential biases in machine learning model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
