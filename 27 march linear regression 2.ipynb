{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba8c7cd0",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
    "\n",
    "Ans:- R-sq is a process of calculating the performance of regression model, we have MSE, MAB RMSE but gives the values in large range, and we cannot compare by the absolute values. so we need to find r-sq whose range lies between 0-1 and r2 score = 1-(y_pred-y)/(Y_pred-y_mean). \n",
    "\n",
    "so we can see that r2 score compare the model performance with the base model which gives output as average of prediction.\n",
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared. \n",
    "\n",
    "Ans:- as number of features increases our base model performance does not change but model get overfitted hence its performance increases, to counter this overfitting, we use adjuster r_sq method to penalise the addition of features.\n",
    "Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - k - 1)]\n",
    "\n",
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "Ans:- When the number of features are large we use r_sq method.\n",
    "\n",
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
    "\n",
    "Ans:- RMSE: it is Root mean square error = √[Σ(yi - ŷi)² / n]\n",
    "\n",
    "MSE : mean square error =  Σ(yi - ŷi)² / n\n",
    "\n",
    "MAE: mean absolute error =  Σ|yi - ŷi| / n\n",
    "\n",
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "\n",
    "Ans:- When we have to decrease error and penalize error to a large extent we use MSE but if outliers are there then whole model get disturbe and gives error as even few outliers creats large changes in model as square of errors are used in cost function. In RMSE the root term is used outlier penalizing effect is reduced to some extent. in MAE we have reduce the effect of penalizing error by removing square term hence it is better to use MEA when data have lot of outliers.\n",
    "\n",
    "\n",
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "\n",
    "Ans:- Lasso regularization is used by adding L1 regularisation term in cost function, it reduce overfitting and also removes unnecessary features, where Ridge use L2 regularisation term , it reduce overfitting only but not reduce the features to zero.\n",
    "\n",
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "\n",
    "Ans: Regularized linear models, such as Ridge Regression and Lasso Regression, help to prevent overfitting in machine learning by adding a penalty term to the cost function. This penalty term reduces the magnitude of the coefficients of the model, thereby simplifying the model and reducing its complexity. By reducing the complexity of the model, regularized linear models help to prevent overfitting, which is the tendency of the model to fit the training data too closely, resulting in poor performance on new, unseen data.\n",
    "\n",
    "For example, let's say we have a dataset with 1000 features and 5000 observations, and we want to build a linear regression model to predict the target variable. Without regularization, the model may overfit the data by fitting the noise in the training data, leading to poor performance on new data. To prevent overfitting, we can use Ridge Regression or Lasso Regression.\n",
    "\n",
    "Ridge Regression adds a penalty term to the cost function that is proportional to the square of the magnitude of the coefficients. This penalty term shrinks the coefficients towards zero, but does not set them exactly to zero. This helps to reduce the complexity of the model and prevent overfitting.\n",
    "\n",
    "Lasso Regression, on the other hand, adds a penalty term to the cost function that is proportional to the absolute value of the coefficients. This penalty term can set some of the coefficients to exactly zero, effectively performing feature selection and reducing the complexity of the model.\n",
    "\n",
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "Ans:- Generally models are non linear with each other hence it is not possible to find the best fit line. \n",
    "\n",
    "Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
    "Model \n",
    "\n",
    "Ans:- it is not possible to say which model is better, after assuming that the outliers are not very large we can say that Model B with MAE = 8 is performing better then RMS =10\n",
    "\n",
    "\n",
    "For example, MAE may not be sensitive to large errors or outliers, as it only measures the average absolute difference. In contrast, RMSE may be more sensitive to outliers or large errors, as it squares the errors, which can result in larger penalties. Therefore, it is important to consider the specific characteristics of the data and the problem being solved when choosing an evaluation metric.\n",
    "\n",
    "\n",
    "Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?\n",
    "\n",
    "Ans:- Ans depends on the dataset, if the dataset have more features and few are irrivilant, then lasso works well, if the features are small and all features are relivent then ridge works well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
