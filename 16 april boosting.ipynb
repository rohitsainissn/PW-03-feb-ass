{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "119efc82",
   "metadata": {},
   "source": [
    "Q1. What is boosting in machine learning?\n",
    "\n",
    "Ans:- boosting is a process to use multiple week decision trees (less depth) arranged in series so that the overall model will results in low byas and low variance.\n",
    "The final output is combination of the results of every models.\n",
    "Main aim of boosting is to reduce the bias.\n",
    "\n",
    "Q2. What are the advantages and limitations of using boosting techniques?\n",
    "\n",
    "Ans: Advantages:-\n",
    "boosting reduces the bias hence accuracy is increased.\n",
    "\n",
    "Boosting can improve the accuracy of a weak model by combining multiple weak models, thereby creating a strong model.\n",
    "\n",
    "Boosting can handle complex data sets with high dimensionality and non-linearity.\n",
    "\n",
    "Boosting can handle missing values in the data set.\n",
    "\n",
    "Boosting is less prone to overfitting than other machine learning techniques.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "Boosting can be computationally expensive, especially when using large data sets.\n",
    "\n",
    "Boosting can be sensitive to noise and outliers in the data set, which can lead to overfitting.\n",
    "\n",
    "Boosting requires careful tuning of the parameters to avoid overfitting or underfitting the model.\n",
    "\n",
    "Boosting can be difficult to interpret, as it involves a combination of multiple models.\n",
    "\n",
    "Boosting can suffer from the problem of bias amplification, where the algorithm may reinforce the bias of the initial weak model.\n",
    "\n",
    "\n",
    "Q3. Explain how boosting works.\n",
    "\n",
    "Ans:- steps involve in boosting:\n",
    "\n",
    "1. first feature is sellected, then a week decision tree is made and its information gain is calculated.\n",
    "2. repeat this process with every feature and sellect the feature with best information gain.\n",
    "3. we drop the sellected feature and repeat step 1 and 2 such that every feature is sellected (we make window of data such that next model will get mostly the wrong predictions of the previous model) .\n",
    "4. to get the output, we consider all the output of every decision tree with their weights.\n",
    "\n",
    "\n",
    "Q4. What are the different types of boosting algorithms?\n",
    "\n",
    "Ans: There are many type of boosting algorithms like gradient boost, XG boost, extreem gradient boost.\n",
    "\n",
    "\n",
    "Q5. What are some common parameters in boosting algorithms?\n",
    "\n",
    "Ans:- common parameters of boosting algorithms are depth of tree, pruning value, learning rate, Regularization parameters, number of estimaters etc\n",
    "\n",
    "\n",
    "Q6. How do boosting algorithms combine weak learners to create a strong learner?\n",
    "\n",
    "Ans:- first we sellect the feature and train it with weak learner, as we sellect the best feature, then many values get correctly classified, those which are incorrectly classified, we send them to next decision tree, this decision tree then make prediction of these wrong classified data and tries to correctly classify them, and those left wrongly classified will we sent to next decision tree, as a result most of the values get correctly classified.\n",
    "\n",
    "\n",
    "Q7. Explain the concept of AdaBoost algorithm and its working.\n",
    "\n",
    "Ans:- \n",
    "1. First we give same weight to every column.\n",
    "\n",
    "2. AdaBoost algorithm sellects the feature to train decision tree with 1 depts only called stump with maximum information gain.\n",
    "3. then we calculate total error and performance of stump.\n",
    "4. new sample weight gets updated such that incorrect rows gets higher weight\n",
    "5. Buckets are formed on basis of the weights as a result higher proportion of wrong value is transfered to next stump\n",
    "6. Drop that feature and repeat the step 1-5 to make further stumps.\n",
    "3. Use these output of all stump, to predict the result which is equall to sum of product of performance of stump with output of stump. \n",
    "\n",
    "Q8. What is the loss function used in AdaBoost algorithm?\n",
    "\n",
    "Ans: The AdaBoost algorithm is primarily based on the boosting principle, which aims to minimize the exponential loss function, also known as the AdaBoost loss function.\n",
    "\n",
    "Q9. How does the AdaBoost algorithm update the weights of misclassified samples?\n",
    "\n",
    "Ans adaboost updates its weight besed on total error produced by a stump, which is sum of miss classified point/total point\n",
    "\n",
    "then performance of stump is calculated : - 1/2 loge (1-total error)/total error\n",
    "\n",
    "then based on this performance weights gets updated = old wt * e^-alpha for correct wt\n",
    "\n",
    "weights gets updated = old wt * e^+alpha for incorrect wt\n",
    "\n",
    "Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?\n",
    "\n",
    "Ans:-  The AdaBoost algorithm can continue to improve the model's accuracy by adding more estimators, but there is a point where the benefits of additional estimators start to diminish, and the model may begin to overfit the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aea6d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
