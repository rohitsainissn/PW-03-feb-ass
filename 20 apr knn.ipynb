{
 "cells": [
  {
   "cell_type": "raw",
   "id": "dffa16fa",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "Ans:- In Knn algorithm we ploy the points on a n dimension graph based on the value of independent features.\n",
    "\n",
    "to calculate the value of test point, plot the point, , find the k nearest neighbours, then the average values of those k nearest point is the regression result of test point\n",
    "\n",
    "In case of classification the majority outcom of k nearrest point is the predection of test point.\n",
    "\n",
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "Ans:- The value of K can be found by hyperparameter testing, we use the cross validation set to test the performance of the range of k value in knn model, the value of k which gives the best performance is sellected.\n",
    "\n",
    "\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "Ans:- The average values of k nearest point is the regression result of test point.  In case of classification the majority outcome of k nearrest point is the predection of test point.\n",
    "\n",
    "\n",
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "The performance of knn classification can be measured by accuracy, precision, recall, confussion matrix.\n",
    "\n",
    "The knn regression model performance can be measured by R2 score and adjusted R2 score.\n",
    "\n",
    "\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "Ans:- The curse of dimensionality refers to the difficulty of making accurate predictions or classifications with K-Nearest Neighbors (KNN) algorithm in high-dimensional data. KNN algorithm uses the distance between samples to classify them. In high-dimensional data, the distance between samples becomes increasingly similar, making it difficult to distinguish between them.\n",
    "\n",
    "As the number of dimensions increases, the amount of data required to cover the feature space increases exponentially. This means that the number of samples needed to cover the space becomes unmanageably large, and the algorithm may become too computationally expensive to use. Additionally, with high dimensional data, there may be a lot of noise and irrelevant features, which can negatively affect the performance of KNN.\n",
    "\n",
    "Therefore, when working with high-dimensional data, it is important to carefully consider the feature selection and data preprocessing methods to reduce the dimensionality of the data, and improve the performance of KNN. Other algorithms such as support vector machines (SVM) or decision trees may be more appropriate for high-dimensional data.\n",
    "\n",
    "\n",
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "Ans:- Deleting missing values if missing values are small.\n",
    "\n",
    "Replace by mean , median or mode.\n",
    "\n",
    "we can use independent variables to predict the missing values,\n",
    "\n",
    "We can cluster the data and provide the mean, median or mode value of the cluster.\n",
    "\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n",
    "\n",
    "Ans:- KNN classifier is better suited for problems where the goal is to classify instances into discrete classes, such as predicting whether an email is spam or not, while KNN regressor is better suited for problems where the goal is to predict continuous values, such as predicting the price of a house based on its features.\n",
    "\n",
    "\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n",
    "\n",
    "Ans:- Strengths:\n",
    "\n",
    "1) KNN is a non-parametric algorithm, which means it does not make any assumptions about the underlying distribution of the data. This makes it more flexible and suitable for a wide range of applications.\n",
    "\n",
    "2) Simple and easy to implement.\n",
    "\n",
    "3) No training phase.\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "Computationally expensive.\n",
    "\n",
    "Sensitive to the choice of distance metric and k.\n",
    "\n",
    "KNN may not perform well on imbalanced datasets.\n",
    "\n",
    "To address the weaknesses of KNN algorithm, the following strategies can be used:\n",
    "\n",
    "Feature selection.\n",
    "\n",
    "Experimenting with different distance metrics can help find the best one for the given problem.\n",
    "\n",
    "Preprocessing techniques such as feature scaling, outlier removal, and handling of missing values can improve the performance of KNN.\n",
    "\n",
    "Ensembling: Ensembling techniques such as bagging and boosting can be used to improve the performance of KNN by combining multiple models.\n",
    "\n",
    "\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "Ans_ Euclidean distance is the shortest distance between two point, measured by ((X2-X1)^2+(Y2-Y1)^2)^.5 , while mangattan distance is |(x1-x2)| + |(y1-y2)|.\n",
    "Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "Ans:- KNN algorithm calculates the distance between instances based on the features, and if the features are not on the same scale, then the distance metric may be dominated by the feature with the largest scale, resulting in poor performance of the algorithm. For example, if one feature is measured in the range of 0-1 and another feature is measured in the range of 0-100, then the latter feature will have a greater impact on the distance metric, even if it is less important for the prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
