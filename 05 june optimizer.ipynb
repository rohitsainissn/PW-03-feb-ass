{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78cab6d8",
   "metadata": {},
   "source": [
    "Q What is the role of optimization algorithms in artificial neural networksK Why are they necessary\n",
    "\n",
    "optimization algorithms are necessary in artificial neural networks to efficiently search for optimal sets of parameters, enable gradient-based optimization, handle non-convex landscapes, improve generalization, and ensure scalability and efficiency in training processes. They form a critical component of the training pipeline for ANNs and help in achieving better performance and convergence.\n",
    "\n",
    "Q Explain the concept of gradient descent and its variants. Discuss their differences and tradeoffs in terms of convergence speed and memory requirements.\n",
    "\n",
    "Gradient descent is an optimization algorithm used in machine learning to find the optimal values for a function's parameters. It iteratively updates the parameters by moving in the direction of steepest descent of the cost function, using the gradient of the function. Variants of gradient descent include  Gradient Descent, which computes gradients using the entire dataset, Stochastic Gradient Descent (SGD), which uses a single random sample, and Mini-Batch Gradient Descent (MBGD), which uses a small batch of samples. GD has slow convergence but low memory requirements, SGD converges faster but has higher memory requirements, while MBGD strikes a balance between the two by processing a batch of samples.\n",
    "\n",
    "\n",
    " Q- Describe the challenges associated with traditional gradient descent optimization methods (e.g., slow\n",
    "convergence, local minima). How do modern optimizers address these challenges\n",
    "\n",
    "Traditional gradient descent optimization methods, such as batch gradient descent, face challenges such as slow convergence and getting stuck in local minima. Slow convergence arises from the need to process the entire training dataset in each iteration, making it computationally expensive for large datasets. Local minima pose a problem as the optimization process might converge to suboptimal solutions.\n",
    "\n",
    "Modern optimizers address these challenges by introducing techniques like stochasticity, adaptive learning rates, and momentum. Stochastic gradient descent (SGD) and mini-batch gradient descent speed up convergence by processing random subsets of data or individual samples. Adaptive optimizers, like AdaGrad, RMSprop, and Adam, adjust the learning rate for each parameter, allowing faster progress in relevant directions and improved handling of sparse data. Momentum methods accumulate past gradients to maintain velocity and escape local minima. These advancements mitigate the challenges associated with traditional gradient descent, leading to faster convergence and increased likelihood of finding better solutions.\n",
    "\n",
    "Q- Discuss the concepts of momentum and learning rate in the context of optimization algorithms. How do\n",
    "they impact convergence and model performance\n",
    "\n",
    "Momentum in optimization algorithms accumulates a fraction of previous updates, accelerating convergence and improving trajectory smoothness. It helps overcome local minima and speeds up convergence. The learning rate determines the step size for parameter updates. A higher learning rate can lead to faster convergence but risks overshooting, while a lower rate ensures stability but may slow convergence. It also affects sensitivity to local minima and noise robustness. Both momentum and learning rate are crucial hyperparameters, requiring careful tuning for optimal performance. Monitoring convergence behavior and model performance helps determine suitable values, balancing convergence speed and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56ced91",
   "metadata": {},
   "source": [
    "Part 2: Optimizer Technique\n",
    "Q- Explain the concept of Stochastic gradient Descent (SGD) and its advantages compared to traditional\n",
    "gradient descent. Discuss its limitations and scenarios where it is most suitablen\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is an optimization algorithm that randomly selects individual training samples to compute the gradient and update the model parameters. Unlike traditional gradient descent, which processes the entire dataset, SGD offers several advantages. It reduces computational requirements and memory usage, as it only operates on one sample at a time. SGD can converge faster per iteration, making it suitable for large-scale datasets. However, it comes with limitations such as noisy updates, slower convergence due to high variance, and difficulties in finding the exact global minimum. SGD is well-suited for scenarios with large datasets, non-convex optimization problems, and when computational resources are limited.\n",
    "\n",
    "Q- Describe the concept of Adam optimizer and how it combines momentum and adaptive learning rates.\n",
    "Discuss its benefits and potential drawbacks.\n",
    "\n",
    "Adam (Adaptive Moment Estimation) is an optimization algorithm that combines the benefits of both momentum and adaptive learning rates. It maintains a running average of both the first-order moment (mean) and the second-order moment (uncentered variance) of the gradients. By incorporating momentum, Adam enables faster convergence and helps navigate regions with high curvature. The adaptive learning rates in Adam adjust the step sizes for each parameter individually, based on the estimated moments. This allows it to handle varying sensitivities of parameters and different scales of gradients. Adam offers benefits such as fast convergence, efficiency, and robustness to different optimization landscapes. However, it can be sensitive to hyperparameter choices and may exhibit slower convergence in certain cases.\n",
    "\n",
    "Q- Explain the concept of RMSprop optimizer and how it addresses the challenges of adaptive learning\n",
    "rates. ompare it with Adam and discuss their relative strengths and weaknesses.\n",
    "\n",
    "RMSprop is an optimization algorithm that addresses the challenges of adaptive learning rates by adapting the step sizes based on the root mean square (RMS) of past gradients. It maintains a running average of the squared gradients and divides the learning rate by this RMS value during parameter updates. This allows it to scale the learning rates individually for each parameter based on their historical gradients.\n",
    "\n",
    "When comparing RMSprop with Adam, both algorithms utilize adaptive learning rates, but they differ in terms of the moments they estimate. RMSprop only considers the second-order moment (squared gradients), while Adam considers both the first-order moment (mean) and second-order moment.\n",
    "\n",
    "Strengths of RMSprop:\n",
    "\n",
    "Robustness: RMSprop is known for its robustness to different optimization landscapes and hyperparameter choices.\n",
    "Efficiency: It is computationally efficient and requires minimal memory, making it suitable for large-scale datasets.\n",
    "Weaknesses of RMSprop:\n",
    "\n",
    "Lack of Momentum: RMSprop does not incorporate explicit momentum terms like Adam, which can affect its ability to navigate through flat regions and accelerate convergence.\n",
    "Hyperparameter Sensitivity: RMSprop's performance can be sensitive to the choice of hyperparameters, such as the learning rate and decay rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf84e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdad279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8132afa5",
   "metadata": {},
   "source": [
    "Part 3: Applyiog Optimiaer\n",
    "Q- Ã…n Implement SD, Adam, and RMSprop optimizers in a deep learning model using a framework of your\n",
    "choice. Train the model on a suitable dataset and compare their impact on model convergence and\n",
    "performancen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41146a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 13:16:33.212305: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import models, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e42a3957",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wine.csv')\n",
    "df['quality'] = df['quality'].replace({'good': 1, 'bad': 0})\n",
    "\n",
    "X = df.drop('quality', axis=1)\n",
    "y= df.quality\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_test,X_val,y_test, y_val = train_test_split(X_test, y_test, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e89d445c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "10/10 [==============================] - 3s 36ms/step - loss: 0.6701 - accuracy: 0.6654 - val_loss: 0.6249 - val_accuracy: 0.7188\n",
      "Epoch 2/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.6011 - accuracy: 0.7420 - val_loss: 0.5291 - val_accuracy: 0.7969\n",
      "Epoch 3/40\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5422 - accuracy: 0.7373 - val_loss: 0.4684 - val_accuracy: 0.8125\n",
      "Epoch 4/40\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.5176 - accuracy: 0.7428 - val_loss: 0.4736 - val_accuracy: 0.8125\n",
      "Epoch 5/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.4989 - accuracy: 0.7568 - val_loss: 0.4613 - val_accuracy: 0.7812\n",
      "Epoch 6/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.4929 - accuracy: 0.7748 - val_loss: 0.4443 - val_accuracy: 0.8281\n",
      "Epoch 7/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.4782 - accuracy: 0.7795 - val_loss: 0.4404 - val_accuracy: 0.8281\n",
      "Epoch 8/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.4640 - accuracy: 0.7826 - val_loss: 0.4381 - val_accuracy: 0.8281\n",
      "Epoch 9/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.4562 - accuracy: 0.7834 - val_loss: 0.4364 - val_accuracy: 0.8281\n",
      "Epoch 10/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.4463 - accuracy: 0.8030 - val_loss: 0.4301 - val_accuracy: 0.7969\n",
      "Epoch 11/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.4358 - accuracy: 0.7975 - val_loss: 0.4245 - val_accuracy: 0.7969\n",
      "Epoch 12/40\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.4219 - accuracy: 0.8116 - val_loss: 0.4281 - val_accuracy: 0.8281\n",
      "Epoch 13/40\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.4250 - accuracy: 0.8038 - val_loss: 0.4197 - val_accuracy: 0.8125\n",
      "Epoch 14/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.4073 - accuracy: 0.8194 - val_loss: 0.4207 - val_accuracy: 0.8125\n",
      "Epoch 15/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.4011 - accuracy: 0.8241 - val_loss: 0.4156 - val_accuracy: 0.8281\n",
      "Epoch 16/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.3840 - accuracy: 0.8327 - val_loss: 0.4172 - val_accuracy: 0.7812\n",
      "Epoch 17/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.3771 - accuracy: 0.8280 - val_loss: 0.4206 - val_accuracy: 0.7812\n",
      "Epoch 18/40\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3649 - accuracy: 0.8374 - val_loss: 0.3989 - val_accuracy: 0.7969\n",
      "Epoch 19/40\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.3549 - accuracy: 0.8460 - val_loss: 0.4212 - val_accuracy: 0.7812\n",
      "Epoch 20/40\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.3481 - accuracy: 0.8421 - val_loss: 0.4123 - val_accuracy: 0.7969\n",
      "Epoch 21/40\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.3486 - accuracy: 0.8514 - val_loss: 0.4360 - val_accuracy: 0.8125\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                768       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,617\n",
      "Trainable params: 23,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)\n",
    "y_train = np.array(y_train).reshape(-1, 1)\n",
    "y_val = np.array(y_val).reshape(-1, 1)\n",
    "y_test = np.array(y_test).reshape(-1,1)\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import  EarlyStopping\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64*2, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64/2, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=40, batch_size=128, validation_data=(X_val, y_val),callbacks=[early_stopping_callback])\n",
    "\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b90f8383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4958 - accuracy: 0.7891\n",
      "Test Loss: 0.49579155445098877\n",
      "Test Accuracy: 0.7890625\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the test loss and accuracy\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c85636e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "10/10 [==============================] - 2s 33ms/step - loss: 0.6473 - accuracy: 0.6646 - val_loss: 0.5524 - val_accuracy: 0.7969\n",
      "Epoch 2/40\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.5420 - accuracy: 0.7326 - val_loss: 0.4672 - val_accuracy: 0.7812\n",
      "Epoch 3/40\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.5133 - accuracy: 0.7506 - val_loss: 0.4520 - val_accuracy: 0.7812\n",
      "Epoch 4/40\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.5005 - accuracy: 0.7545 - val_loss: 0.4464 - val_accuracy: 0.8125\n",
      "Epoch 5/40\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.4968 - accuracy: 0.7608 - val_loss: 0.4494 - val_accuracy: 0.8125\n",
      "Epoch 6/40\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4835 - accuracy: 0.7592 - val_loss: 0.4453 - val_accuracy: 0.8125\n",
      "Epoch 7/40\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.4802 - accuracy: 0.7725 - val_loss: 0.4388 - val_accuracy: 0.8125\n",
      "Epoch 8/40\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4737 - accuracy: 0.7795 - val_loss: 0.4450 - val_accuracy: 0.7812\n",
      "Epoch 9/40\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.4649 - accuracy: 0.7795 - val_loss: 0.4437 - val_accuracy: 0.7969\n",
      "Epoch 10/40\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4631 - accuracy: 0.7826 - val_loss: 0.4339 - val_accuracy: 0.8125\n",
      "Epoch 11/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.4541 - accuracy: 0.7881 - val_loss: 0.4345 - val_accuracy: 0.8125\n",
      "Epoch 12/40\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.4454 - accuracy: 0.7928 - val_loss: 0.4388 - val_accuracy: 0.8438\n",
      "Epoch 13/40\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.4372 - accuracy: 0.7928 - val_loss: 0.4442 - val_accuracy: 0.8125\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 64)                768       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,617\n",
      "Trainable params: 23,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = models.Sequential()\n",
    "model2.add(layers.Dense(64, activation='relu'))\n",
    "model2.add(layers.Dense(64, activation='relu'))\n",
    "model2.add(layers.Dense(64*2, activation='relu'))\n",
    "model2.add(layers.Dense(64, activation='relu'))\n",
    "model2.add(layers.Dense(64/2, activation='relu'))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "model2.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history2 = model2.fit(X_train, y_train, epochs=40, batch_size=128, validation_data=(X_val, y_val),callbacks=[early_stopping_callback])\n",
    "\n",
    "\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ced1c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4912 - accuracy: 0.7383\n",
      "Test Loss: 0.49117597937583923\n",
      "Test Accuracy: 0.73828125\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model2.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the test loss and accuracy\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aeee097",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "10/10 [==============================] - 1s 34ms/step - loss: 0.6907 - accuracy: 0.5489 - val_loss: 0.6830 - val_accuracy: 0.6094\n",
      "Epoch 2/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.6886 - accuracy: 0.5504 - val_loss: 0.6806 - val_accuracy: 0.6094\n",
      "Epoch 3/40\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6865 - accuracy: 0.5536 - val_loss: 0.6783 - val_accuracy: 0.6094\n",
      "Epoch 4/40\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.6844 - accuracy: 0.5559 - val_loss: 0.6758 - val_accuracy: 0.6094\n",
      "Epoch 5/40\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.6824 - accuracy: 0.5582 - val_loss: 0.6734 - val_accuracy: 0.5938\n",
      "Epoch 6/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.6803 - accuracy: 0.5622 - val_loss: 0.6708 - val_accuracy: 0.5938\n",
      "Epoch 7/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.6781 - accuracy: 0.5684 - val_loss: 0.6682 - val_accuracy: 0.5938\n",
      "Epoch 8/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.6760 - accuracy: 0.5684 - val_loss: 0.6655 - val_accuracy: 0.5938\n",
      "Epoch 9/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.6739 - accuracy: 0.5747 - val_loss: 0.6628 - val_accuracy: 0.5938\n",
      "Epoch 10/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.6718 - accuracy: 0.5856 - val_loss: 0.6600 - val_accuracy: 0.5781\n",
      "Epoch 11/40\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.6696 - accuracy: 0.5864 - val_loss: 0.6571 - val_accuracy: 0.5938\n",
      "Epoch 12/40\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.6675 - accuracy: 0.5911 - val_loss: 0.6542 - val_accuracy: 0.5938\n",
      "Epoch 13/40\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.6653 - accuracy: 0.5981 - val_loss: 0.6512 - val_accuracy: 0.5781\n",
      "Epoch 14/40\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.6630 - accuracy: 0.6013 - val_loss: 0.6482 - val_accuracy: 0.5781\n",
      "Epoch 15/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.6607 - accuracy: 0.6099 - val_loss: 0.6450 - val_accuracy: 0.5938\n",
      "Epoch 16/40\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.6582 - accuracy: 0.6091 - val_loss: 0.6418 - val_accuracy: 0.5938\n",
      "Epoch 17/40\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.6557 - accuracy: 0.6200 - val_loss: 0.6385 - val_accuracy: 0.6094\n",
      "Epoch 18/40\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.6531 - accuracy: 0.6435 - val_loss: 0.6352 - val_accuracy: 0.6094\n",
      "Epoch 19/40\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.6505 - accuracy: 0.6607 - val_loss: 0.6317 - val_accuracy: 0.6094\n",
      "Epoch 20/40\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.6476 - accuracy: 0.6630 - val_loss: 0.6282 - val_accuracy: 0.6406\n",
      "Epoch 21/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.6447 - accuracy: 0.6693 - val_loss: 0.6247 - val_accuracy: 0.6875\n",
      "Epoch 22/40\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.6418 - accuracy: 0.6818 - val_loss: 0.6210 - val_accuracy: 0.7188\n",
      "Epoch 23/40\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.6386 - accuracy: 0.6912 - val_loss: 0.6172 - val_accuracy: 0.7188\n",
      "Epoch 24/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.6354 - accuracy: 0.6974 - val_loss: 0.6132 - val_accuracy: 0.7188\n",
      "Epoch 25/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.6321 - accuracy: 0.6998 - val_loss: 0.6091 - val_accuracy: 0.7031\n",
      "Epoch 26/40\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.6286 - accuracy: 0.7029 - val_loss: 0.6048 - val_accuracy: 0.7031\n",
      "Epoch 27/40\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.6251 - accuracy: 0.7084 - val_loss: 0.6004 - val_accuracy: 0.7031\n",
      "Epoch 28/40\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6216 - accuracy: 0.7146 - val_loss: 0.5959 - val_accuracy: 0.7188\n",
      "Epoch 29/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.6179 - accuracy: 0.7115 - val_loss: 0.5915 - val_accuracy: 0.7188\n",
      "Epoch 30/40\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6144 - accuracy: 0.7146 - val_loss: 0.5871 - val_accuracy: 0.7188\n",
      "Epoch 31/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.6106 - accuracy: 0.7170 - val_loss: 0.5827 - val_accuracy: 0.7188\n",
      "Epoch 32/40\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.6068 - accuracy: 0.7154 - val_loss: 0.5781 - val_accuracy: 0.7188\n",
      "Epoch 33/40\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.6030 - accuracy: 0.7185 - val_loss: 0.5735 - val_accuracy: 0.7188\n",
      "Epoch 34/40\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.5990 - accuracy: 0.7224 - val_loss: 0.5688 - val_accuracy: 0.7031\n",
      "Epoch 35/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.5952 - accuracy: 0.7248 - val_loss: 0.5641 - val_accuracy: 0.7031\n",
      "Epoch 36/40\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5912 - accuracy: 0.7248 - val_loss: 0.5593 - val_accuracy: 0.7031\n",
      "Epoch 37/40\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5873 - accuracy: 0.7248 - val_loss: 0.5547 - val_accuracy: 0.7188\n",
      "Epoch 38/40\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5835 - accuracy: 0.7303 - val_loss: 0.5500 - val_accuracy: 0.7188\n",
      "Epoch 39/40\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5797 - accuracy: 0.7326 - val_loss: 0.5455 - val_accuracy: 0.7188\n",
      "Epoch 40/40\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5760 - accuracy: 0.7326 - val_loss: 0.5411 - val_accuracy: 0.7031\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 64)                768       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,617\n",
      "Trainable params: 23,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = models.Sequential()\n",
    "model3.add(layers.Dense(64, activation='relu'))\n",
    "model3.add(layers.Dense(64, activation='relu'))\n",
    "model3.add(layers.Dense(64*2, activation='relu'))\n",
    "model3.add(layers.Dense(64, activation='relu'))\n",
    "model3.add(layers.Dense(64/2, activation='relu'))\n",
    "model3.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history3 = model3.fit(X_train, y_train, epochs=40, batch_size=128, validation_data=(X_val, y_val),callbacks=[early_stopping_callback])\n",
    "\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08c96075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5760 - accuracy: 0.7031\n",
      "Test Loss: 0.5759593844413757\n",
      "Test Accuracy: 0.703125\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model3.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the test loss and accuracy\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f3a8e",
   "metadata": {},
   "source": [
    "in our experiment it is clear that adam gives the highest accuracy and sgd least, sgd also converges very slowly as compare to others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a7c416",
   "metadata": {},
   "source": [
    "Q- Discuss the considerations and tradeoffs when choosing the appropriate optimizer for a given neural\n",
    "network architecture and task. onsider factors such as convergence speed, stability, and generalization performance.\n",
    "\n",
    "When selecting an optimizer for a neural network architecture and task, it's important to consider several factors and tradeoffs:\n",
    "\n",
    "Convergence Speed: Different optimizers have varying convergence speeds. Optimizers like Adam and RMSprop often converge faster compared to basic gradient descent or SGD. If you have limited computational resources or need faster convergence, choosing an optimizer with a faster convergence rate can be advantageous.\n",
    "\n",
    "Stability: The stability of the optimizer during training is crucial. Some optimizers, such as SGD with momentum, help stabilize the training process by reducing oscillations and avoiding getting stuck in local minima. Stability leads to smoother loss curves and more consistent updates to the model parameters.\n",
    "\n",
    "Generalization Performance: Generalization refers to the model's ability to perform well on unseen data. Different optimizers can impact generalization performance. Regularization techniques, such as weight decay or dropout, can aid in improving generalization. Some optimizers, like SGD with weight decay or Adam, inherently incorporate regularization, resulting in better generalization.\n",
    "\n",
    "Robustness to Noise and Sparse Gradients: In scenarios with noisy or sparse gradients, adaptive optimizers like Adam, RMSprop, or AdaGrad can be beneficial. These optimizers adjust the learning rate for each parameter individually based on historical gradients, enhancing their ability to handle noise and sparse data.\n",
    "\n",
    "Computational Efficiency: Optimizers can vary in their computational requirements. Traditional gradient descent, SGD, and momentum-based methods tend to be computationally efficient as they process one or small batches of samples at a time. On the other hand, adaptive optimizers like Adam or AdaGrad may involve additional computations and memory due to per-parameter information maintenance.\n",
    "\n",
    "Hyperparameter Sensitivity: Optimizers often have hyperparameters that need to be tuned, such as learning rate, momentum, or decay rates. The sensitivity of these hyperparameters can differ across optimizers. Some optimizers may be more forgiving and less sensitive to hyperparameter choices, while others require careful tuning for optimal performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fd27d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
