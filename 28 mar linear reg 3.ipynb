{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec599699",
   "metadata": {},
   "source": [
    "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?\n",
    "\n",
    "Ans:- difference between ridge regression and OLS regression is that ridge regression introduces a penalty term to the objective function to shrink the coefficients towards zero and reduce overfitting, while OLS regression estimates the coefficients by minimizing the sum of squared residuals without any penalty term.\n",
    "\n",
    "Q2. What are the assumptions of Ridge Regression?\n",
    "Ans:- Linearity: The relationship between the independent variables and the dependent variable should be linear.\n",
    "\n",
    "Independence of errors: The errors should be independent of each other, and there should be no autocorrelation in the residuals.\n",
    "\n",
    "Normality of errors: The errors should be normally distributed.\n",
    "\n",
    "Homoscedasticity: The variance of the errors should be constant across all levels of the independent variables.\n",
    "\n",
    "the independent variables are not perfectly correlated with each other. This assumption is important because the penalty term in ridge regression is based on the sum of squared coefficients, and if the independent variables are highly correlated, the penalty term will be too large, leading to biased estimates of the coefficients.\n",
    "\n",
    "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n",
    "\n",
    "Ans:- Tuning parameter is used to control regularization term, if we increase lambda then the regulation term increase, the model will focus more on reducing slop square hence chances of underfit increase, on the other hand if we decrease lambda, our error term decreases and chances of overfitting increases.\n",
    "\n",
    "Q4. Can Ridge Regression be used for feature selection? If yes, how?\n",
    "\n",
    "Ans:- Yes, in Ridge regulizer term used to shrink the coefficient, and we can sellect a threshold such that below which the coefficient value will be considered as zero\n",
    "\n",
    "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?\n",
    "\n",
    "Ans:- Ridge Regression can help to mitigate the effects of multicollinearity by shrinking the regression coefficients towards zero. This can help to reduce the variance of the estimates and improve the stability of the model. \n",
    "\n",
    "\n",
    "Q6. Can Ridge Regression handle both categorical and continuous independent variables?\n",
    "\n",
    "Ans:- Yes Redge Regression handle both categorical and continuous independent variables, we have to clean the data and make the continuous variable in digits and categorical variables in dummy encoding.\n",
    "\n",
    "Q7. How do you interpret the coefficients of Ridge Regression?\n",
    "\n",
    "Ans:- interpretion is similar to linear regression, in Ridge regression coefficients are much smaller, higher the cofficient high important the feature is, if coefficient is negative then the relation of the feature with independent variable is inversely proportional. \n",
    "\n",
    "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?\n",
    "\n",
    "Ans:- To use Ridge Regression for time-series data analysis, the data should be first transformed into a suitable format. One common approach is to use a sliding window technique, where the time-series data is divided into smaller segments, and each segment is used to train a separate Ridge Regression model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
