{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4035315",
   "metadata": {},
   "source": [
    "Q-1 Explain the concept of batch normalization in the context of Artificial Neural Networks\n",
    "\n",
    "It is a process of normalising the inputs before entering a layer of deep neural network. It makes the training fast and stable.\n",
    "\n",
    "Q- Describe the benefits of using batch normalization during training.\n",
    "\n",
    "It makes the training faster and smoother.\n",
    "\n",
    "Q- Discuss the working principle of batch normalization, including the normalization step and the learnable parameters.\n",
    "\n",
    "\n",
    "Batch normalization is a technique commonly used in deep neural networks to address the internal covariate shift problem. The internal covariate shift refers to the change in the distribution of intermediate feature values that occurs during the training of a deep neural network. Batch normalization helps to stabilize and improve the training process by normalizing the inputs to each layer.\n",
    "\n",
    "Normalization Step:\n",
    "In batch normalization, the inputs to a layer are normalized by subtracting the mini-batch mean and dividing by the mini-batch standard deviation. The normalization step is performed independently for each feature in the input. Let's consider a mini-batch of size m and a feature dimension of d\n",
    "\n",
    "\n",
    "Learnable Parameters:\n",
    "To allow the network to learn a different representation from the normalized inputs, batch normalization introduces two learnable parameters per feature: γ (scaling parameter) and β (shift parameter).\n",
    "a. Scaling: Each normalized input ẋi is multiplied by the scaling parameter γ. This parameter allows the network to learn the optimal scale for each feature, providing flexibility in the representation. The output after scaling is given by:\n",
    "ȳi = γ * ẋi\n",
    "\n",
    "b. Shifting: The scaled output ȳi is then shifted by the shift parameter β. This parameter allows the network to learn the optimal bias for each feature:\n",
    "ȳi = ȳi + β\n",
    "\n",
    "By incorporating these learnable parameters, batch normalization enables the network to adapt the normalized inputs to the desired scale and bias, effectively restoring the representation power that might have been lost during normalization.\n",
    "\n",
    "During training, both γ and β are learned through backpropagation. The network updates these parameters along with the other parameters in the network to optimize the overall loss function. During inference, the learned parameters are typically fixed, and the batch statistics (mean and standard deviation) calculated during training are used for normalization.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " ImpementatiTion\n",
    "Q-1 Choose a dataset of your choice (e.g., MNIST, CIAR-0) and preprocess it.\n",
    "Q-2 Implement a simple feedforward neural network using any deep learning framework/library (e.g.,Tensorlow, xyTorch)\n",
    "Q-3 Train the neural network on the chosen dataset without using batch normalizationr\n",
    "Q-4 Implement batch normalization layers in the neural network and train the model againr\n",
    "Q-5 Compare the training and validation performance (e.g., accuracy, loss) between the models with and\n",
    "without batch normalizationr\n",
    "Q-6 Discuss the impact of batch normalization on the training process and the performance of the neural\n",
    "network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99a968f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-28 20:53:47.041646: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2abb457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "422/422 [==============================] - 29s 65ms/step - loss: 0.2476 - accuracy: 0.9276 - val_loss: 0.0752 - val_accuracy: 0.9790\n",
      "Epoch 2/10\n",
      "422/422 [==============================] - 29s 70ms/step - loss: 0.0663 - accuracy: 0.9795 - val_loss: 0.0589 - val_accuracy: 0.9837\n",
      "Epoch 3/10\n",
      "422/422 [==============================] - 29s 69ms/step - loss: 0.0475 - accuracy: 0.9855 - val_loss: 0.0450 - val_accuracy: 0.9867\n",
      "Epoch 4/10\n",
      "422/422 [==============================] - 29s 68ms/step - loss: 0.0374 - accuracy: 0.9883 - val_loss: 0.0429 - val_accuracy: 0.9882\n",
      "Epoch 5/10\n",
      "422/422 [==============================] - 28s 67ms/step - loss: 0.0291 - accuracy: 0.9911 - val_loss: 0.0457 - val_accuracy: 0.9870\n",
      "Epoch 6/10\n",
      "422/422 [==============================] - 29s 68ms/step - loss: 0.0251 - accuracy: 0.9924 - val_loss: 0.0406 - val_accuracy: 0.9888\n",
      "Epoch 7/10\n",
      "422/422 [==============================] - 28s 67ms/step - loss: 0.0188 - accuracy: 0.9942 - val_loss: 0.0386 - val_accuracy: 0.9893\n",
      "Epoch 8/10\n",
      "422/422 [==============================] - 28s 66ms/step - loss: 0.0171 - accuracy: 0.9947 - val_loss: 0.0369 - val_accuracy: 0.9905\n",
      "Epoch 9/10\n",
      "422/422 [==============================] - 27s 65ms/step - loss: 0.0135 - accuracy: 0.9956 - val_loss: 0.0373 - val_accuracy: 0.9902\n",
      "Epoch 10/10\n",
      "422/422 [==============================] - 28s 67ms/step - loss: 0.0113 - accuracy: 0.9964 - val_loss: 0.0510 - val_accuracy: 0.9857\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import models, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Normalize the input data\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_val = X_val.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the input data to match the input shape of the model\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 28, 28, 1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], 28, 28, 1))\n",
    "\n",
    "# Convert the target values to integers\n",
    "y_train = y_train.astype('int32')\n",
    "y_val = y_val.astype('int32')\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "417389a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "422/422 [==============================] - 45s 104ms/step - loss: 0.1159 - accuracy: 0.9675 - val_loss: 0.9533 - val_accuracy: 0.6983\n",
      "Epoch 2/10\n",
      "422/422 [==============================] - 44s 104ms/step - loss: 0.0352 - accuracy: 0.9901 - val_loss: 0.0400 - val_accuracy: 0.9895\n",
      "Epoch 3/10\n",
      "422/422 [==============================] - 50s 118ms/step - loss: 0.0228 - accuracy: 0.9935 - val_loss: 0.0399 - val_accuracy: 0.9893\n",
      "Epoch 4/10\n",
      "422/422 [==============================] - 44s 105ms/step - loss: 0.0166 - accuracy: 0.9951 - val_loss: 0.0369 - val_accuracy: 0.9893\n",
      "Epoch 5/10\n",
      "422/422 [==============================] - 45s 107ms/step - loss: 0.0112 - accuracy: 0.9967 - val_loss: 0.0392 - val_accuracy: 0.9885\n",
      "Epoch 6/10\n",
      "422/422 [==============================] - 45s 107ms/step - loss: 0.0079 - accuracy: 0.9978 - val_loss: 0.0494 - val_accuracy: 0.9873\n",
      "Epoch 7/10\n",
      "422/422 [==============================] - 45s 106ms/step - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.0465 - val_accuracy: 0.9878\n",
      "Epoch 8/10\n",
      "422/422 [==============================] - 46s 108ms/step - loss: 0.0076 - accuracy: 0.9976 - val_loss: 0.0390 - val_accuracy: 0.9903\n",
      "Epoch 9/10\n",
      "422/422 [==============================] - 45s 106ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.0444 - val_accuracy: 0.9893\n",
      "Epoch 10/10\n",
      "422/422 [==============================] - 47s 111ms/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.0413 - val_accuracy: 0.9890\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import models, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Normalize the input data\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_val = X_val.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the input data to match the input shape of the model\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 28, 28, 1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], 28, 28, 1))\n",
    "\n",
    "# Convert the target values to integers\n",
    "y_train = y_train.astype('int32')\n",
    "y_val = y_val.astype('int32')\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46fd33b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy slightly increase on using batch normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8d48de",
   "metadata": {},
   "source": [
    "Q3. ExperimentatiTn and ÎnaysisU\n",
    "Sr Experiment with different batch sizes and observe the effect on the training dynamics and model\n",
    "performancer\n",
    "Er Discuss the advantages and potential limitations of batch normalization in improving the training of\n",
    "neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4340ea93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1688/1688 [==============================] - 57s 33ms/step - loss: 0.1079 - accuracy: 0.9686 - val_loss: 0.0711 - val_accuracy: 0.9798\n",
      "Epoch 2/6\n",
      "1688/1688 [==============================] - 55s 33ms/step - loss: 0.0460 - accuracy: 0.9854 - val_loss: 0.0465 - val_accuracy: 0.9858\n",
      "Epoch 3/6\n",
      "1688/1688 [==============================] - 57s 34ms/step - loss: 0.0325 - accuracy: 0.9899 - val_loss: 0.0439 - val_accuracy: 0.9872\n",
      "Epoch 4/6\n",
      "1688/1688 [==============================] - 57s 34ms/step - loss: 0.0251 - accuracy: 0.9918 - val_loss: 0.0410 - val_accuracy: 0.9890\n",
      "Epoch 5/6\n",
      "1688/1688 [==============================] - 57s 34ms/step - loss: 0.0199 - accuracy: 0.9937 - val_loss: 0.0455 - val_accuracy: 0.9880\n",
      "Epoch 6/6\n",
      "1688/1688 [==============================] - 58s 34ms/step - loss: 0.0177 - accuracy: 0.9942 - val_loss: 0.0330 - val_accuracy: 0.9905\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import models, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Normalize the input data\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_val = X_val.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the input data to match the input shape of the model\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 28, 28, 1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], 28, 28, 1))\n",
    "\n",
    "# Convert the target values to integers\n",
    "y_train = y_train.astype('int32')\n",
    "y_val = y_val.astype('int32')\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=6, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8795ca2",
   "metadata": {},
   "source": [
    "on decreasing batch size to 32 from 128, iteration takes longer time, due to overfitting accuracy increase, in 6th epoch accuracy is high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49c2c3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "282/282 [==============================] - 49s 168ms/step - loss: 0.1366 - accuracy: 0.9627 - val_loss: 3.4030 - val_accuracy: 0.1237\n",
      "Epoch 2/12\n",
      "282/282 [==============================] - 50s 178ms/step - loss: 0.0405 - accuracy: 0.9888 - val_loss: 0.2890 - val_accuracy: 0.9142\n",
      "Epoch 3/12\n",
      "282/282 [==============================] - 44s 156ms/step - loss: 0.0258 - accuracy: 0.9929 - val_loss: 0.0404 - val_accuracy: 0.9880\n",
      "Epoch 4/12\n",
      "282/282 [==============================] - 47s 167ms/step - loss: 0.0174 - accuracy: 0.9953 - val_loss: 0.0484 - val_accuracy: 0.9852\n",
      "Epoch 5/12\n",
      "282/282 [==============================] - 46s 164ms/step - loss: 0.0121 - accuracy: 0.9968 - val_loss: 0.0496 - val_accuracy: 0.9858\n",
      "Epoch 6/12\n",
      "282/282 [==============================] - 45s 161ms/step - loss: 0.0085 - accuracy: 0.9981 - val_loss: 0.0337 - val_accuracy: 0.9903\n",
      "Epoch 7/12\n",
      "282/282 [==============================] - 42s 150ms/step - loss: 0.0065 - accuracy: 0.9987 - val_loss: 0.0358 - val_accuracy: 0.9892\n",
      "Epoch 8/12\n",
      "282/282 [==============================] - 45s 160ms/step - loss: 0.0055 - accuracy: 0.9985 - val_loss: 0.0390 - val_accuracy: 0.9887\n",
      "Epoch 9/12\n",
      "282/282 [==============================] - 47s 166ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.0388 - val_accuracy: 0.9900\n",
      "Epoch 10/12\n",
      "282/282 [==============================] - 46s 162ms/step - loss: 0.0045 - accuracy: 0.9989 - val_loss: 0.0469 - val_accuracy: 0.9882\n",
      "Epoch 11/12\n",
      "282/282 [==============================] - 58s 206ms/step - loss: 0.0048 - accuracy: 0.9988 - val_loss: 0.0482 - val_accuracy: 0.9870\n",
      "Epoch 12/12\n",
      "282/282 [==============================] - 46s 163ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.0369 - val_accuracy: 0.9903\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import models, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Normalize the input data\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_val = X_val.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the input data to match the input shape of the model\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 28, 28, 1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], 28, 28, 1))\n",
    "\n",
    "# Convert the target values to integers\n",
    "y_train = y_train.astype('int32')\n",
    "y_val = y_val.astype('int32')\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=12, batch_size=32*6, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f03d0",
   "metadata": {},
   "source": [
    "accuracy remains same with large batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e27f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16290448",
   "metadata": {},
   "source": [
    "\n",
    "Batch normalization (BN) offers several advantages in improving the training of neural networks:\n",
    "\n",
    "Normalization and Stability: BN normalizes the inputs within each mini-batch, ensuring that they have zero mean and unit variance. This normalization helps stabilize the training process, especially in deeper networks. It reduces the internal covariate shift, making the optimization landscape more favorable and enabling faster convergence.\n",
    "\n",
    "Reduced Sensitivity to Initialization: BN reduces the dependence of the model on the weight initialization. It allows the network to converge even when using suboptimal weight initialization methods, which can save time and effort in tuning the initialization.\n",
    "\n",
    "Higher Learning Rates: With BN, higher learning rates can be used during training without the risk of divergence. This is because BN normalizes the gradients flowing through the network, preventing them from becoming too large or too small, which can hinder the learning process.\n",
    "\n",
    "Regularization Effect: BN acts as a form of regularization by introducing a small amount of noise to the inputs within each mini-batch. This noise can help reduce overfitting and improve generalization performance.\n",
    "\n",
    "Reduction in Dependency on Dropout: BN can reduce the need for Dropout regularization, or in some cases, eliminate the need for it altogether. This is because BN provides some inherent regularization effect by reducing internal covariate shift and balancing the activations, which helps prevent overfitting.\n",
    "\n",
    "Despite its advantages, batch normalization has some potential limitations:\n",
    "\n",
    "Increased Computational Complexity: BN adds extra computational overhead during both training and inference. It requires computing the mean and variance of each mini-batch, as well as performing additional computations for normalization and scaling. This can slow down the training process, particularly for large models or when running on resource-constrained devices.\n",
    "\n",
    "Batch Size Dependency: BN performance can be sensitive to the choice of batch size. Very small batch sizes can lead to inaccurate estimates of mean and variance, resulting in suboptimal normalization. It is generally recommended to use larger batch sizes for BN to work effectively.\n",
    "\n",
    "Difficulty with Small Batch Sizes: BN may not work well with very small batch sizes, such as in online learning or certain specialized scenarios. When the batch size is too small, the estimated mean and variance may be less accurate, leading to instability in the normalization process.\n",
    "\n",
    "Inference Dependency on Statistics: During inference, the BN layer requires access to the population statistics (mean and variance) of the entire training dataset. This can be impractical or infeasible in certain deployment scenarios, such as when serving the model on resource-constrained devices or in real-time applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
