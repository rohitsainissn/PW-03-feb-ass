{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6070a907",
   "metadata": {},
   "source": [
    "Q1. Describe the decision tree classifier algorithm and how it works to make predictions.\n",
    "\n",
    "Ans:- Decision tree follow the below steps to make predictions:-\n",
    "1. first feature is sellected, then it splits the data into two branches based on its features.\n",
    "2. Information gain is calculated for the best threshold.\n",
    "3. This process 1-2 is repeated for each feature and the root is made up of best information gain.\n",
    "4. the two child root is now treated as main root separately and step1-3 is followed till complet split is found.\n",
    "\n",
    "Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification.\n",
    "\n",
    "Ans:- first feature is sellected.\n",
    "entropy is calculated by formula Entropy = - (p1 * log2(p1) + p2 * log2(p2) + ... + pn * log2(pn))\n",
    "\n",
    "Then we calculate information gain of every feature with best threshold with the formula\n",
    "Information Gain = Entropy(parent) - [Weighted Average] * Entropy(children)\n",
    "the feature with best information gain is considered and makes the root nood, \n",
    "\n",
    "the two child root is now treated as main root separately and step1-3 is followed till complet split is found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60705f1e",
   "metadata": {},
   "source": [
    "Q3. Explain how a decision tree classifier can be used to solve a binary classification problem.\n",
    "\n",
    "Ans:- first feature is sellected.\n",
    "entropy is calculated by formula Entropy = - (p1 * log2(p1) + p2 * log2(p2) + ... + pn * log2(pn))\n",
    "\n",
    "Then we calculate information gain of every feature with best threshold with the formula\n",
    "Information Gain = Entropy(parent) - [Weighted Average] * Entropy(children)\n",
    "the feature with best information gain is considered and makes the root nood, \n",
    "\n",
    "the two child root is now treated as main root separately and step1-3 is followed till complet split is found.\n",
    "\n",
    "Then we give the query to our decision tree,\n",
    "Based on its features, we reaches to the leave node which will be the answer of the classification problem of the trained decision tree for binary classification.\n",
    "\n",
    "Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make\n",
    "predictions.\n",
    "\n",
    "Ans:- The geometric intuition behind decision tree classification is that the algorithm is essentially dividing the feature space into a series of rectangles or hyper-rectangles, where each rectangle corresponds to a different class. The boundaries between the rectangles are defined by the values of the features, and the goal of the algorithm is to find the boundaries that best separate the different classes.\n",
    "\n",
    "To make a prediction using a decision tree, you start at the root node of the tree and follow a path down the tree based on the values of the features. At each node, you check the value of a particular feature and follow the corresponding branch of the tree. This process continues until you reach a leaf node, which corresponds to a particular class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9586c2ce",
   "metadata": {},
   "source": [
    "Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a\n",
    "classification model.\n",
    "\n",
    "Ans:- Confussion matrix tells the following data in tabular manner shown below:\n",
    "\n",
    "true positive, False Positive\n",
    "\n",
    "Fasle Negative , True Negative\n",
    "\n",
    "True positive are the values positive in reality and predicted positive\n",
    "\n",
    "False positive: predicted positive but negative in reality\n",
    "\n",
    "False negative : predicted negative but positive in reality\n",
    "\n",
    "True Negative: predicted negative and negative in reality\n",
    "\n",
    "We use confusion matrix to find accuracy, precision, recall and F score\n",
    "\n",
    " Precision = TP/(TP+FP)\n",
    "\n",
    "It is performed to know how much positive predicted values are really positive, the cases in which we have to be very sure about if the positive result i.e. FP must me minimum, we use Precision, like in spam detection, before sending a mail to spam box, we must be sure that the email is a spam.\n",
    "\n",
    "Recall= TP/(TP+FN)\n",
    "\n",
    "Recall is performed to make sure that we have not left any positive case without identifing it. Like in cancer detection test, we have to be sure that no person's report is negative if he have the cancer, i.e. we have to reduce FN.\n",
    "\n",
    "Accuracy :- \n",
    "accuracy = TP+TN / TP+TN+FP+FN\n",
    "\n",
    "Specificity: Specificity is the proportion of true negative predictions out of the total number of actual negative cases. It is calculated as TN / (TN + FP).\n",
    "\n",
    "F1 score: F1 score is the harmonic mean of precision and recall, and provides a balanced measure of both metrics. It is calculated as 2 * (precision * recall) / (precision + recall).\n",
    "\n",
    "\n",
    "Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be\n",
    "calculated from it.\n",
    "\n",
    "[112,5]   [TP,FP]\n",
    "\n",
    "[4,98]    [FN,TN]\n",
    "\n",
    "the above matrix is the example of confusion matrix\n",
    "\n",
    "Recall= TP/(TP+FN)\n",
    " Recall = 112/(112+4) = 0.957\n",
    " \n",
    "Recall is performed to make sure that we have not left any positive case without identifing it. Like in cancer detection test, we have to be sure that no person's report is negative if he have the cancer, i.e. we have to reduce FN.\n",
    "\n",
    "\n",
    "Precision = TP/(TP+FP)\n",
    " Precision = 112/(112+5) = 0.966\n",
    " \n",
    "F1 score = 2 * (precision * recall) / (precision + recall)\n",
    " f1 = 2*.957*.977/(.957+.977) = .967\n",
    "\n",
    "It is performed to know how much positive predicted values are really positive, the cases in which we have to be very sure about if the positive result i.e. FP must me minimum, we use Precision, like in spam detection, before sending a mail to spam box, we must be sure that the email is a spam.\n",
    "\n",
    "Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and explain how this can be done.\n",
    "\n",
    "Ans:- It is important to choose proper evaluation metric, if we want to classifify a data where we have to be sure about it is positive or not like in case of span detection we need to be sure, there we use precision because we have to take care of minimum FP.\n",
    "\n",
    "On the other hand if we have a data set like cancer detection where can't take risk of giving negative report of a cancer patient, i.e. FN must be minimum, there we use precision. to ensure FN is minimum,\n",
    "\n",
    "To calculate performance,  we can simply calculate  accuracy = correct prediction/total prediction.\n",
    "\n",
    "\n",
    "Q8. Provide an example of a classification problem where precision is the most important metric, and\n",
    "explain why.\n",
    "\n",
    "if we want to classifify a data where we have to be sure about it is positive or not like in case of span detection we need to be sure, there we use precision because we have to take care of minimum FP.\n",
    "\n",
    "Precision = TP/(TP+FP)\n",
    "\n",
    "\n",
    "Q9. Provide an example of a classification problem where recall is the most important metric, and explain\n",
    "why.\n",
    "\n",
    "Ans:- if we have a data set like cancer detection where can't take risk of giving negative report of a cancer patient, i.e. FN must be minimum, there we use precision. to ensure FN is minimum.\n",
    "\n",
    "Recall= TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293cabb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
