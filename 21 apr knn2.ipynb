{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc9465b",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
    "\n",
    "Ans- Euclidean distance is the shortest distance between two point, measured by ((X2-X1)^2+(Y2-Y1)^2)^.5 , while mangattan distance is |(x1-x2)| + |(y1-y2)|.\n",
    "\n",
    "The Euclidean distance metric works well when the data is distributed uniformly and has equal variance in all directions. It is sensitive to the scale of the features, and therefore, it is important to normalize the data before using this distance metric. On the other hand, the Manhattan distance metric is less sensitive to the scale of the features and works well when the data is distributed along axes, such as in a grid-like pattern.\n",
    "\n",
    "\n",
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "used to determine the optimal k value?\n",
    "\n",
    "Ans:- The value of K can be found by hyperparameter testing, we use the cross validation set to test the performance of the range of k value in knn model, the value of k which gives the best performance is sellected.\n",
    "\n",
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "what situations might you choose one distance metric over the other?\n",
    "\n",
    "Ans:- The choice of distance metric depends on the nature of the data and the problem being solved. In general, Euclidean distance works well for continuous features, and Manhattan distance works well for discrete or categorical features. When the features have different importance, Minkowski distance with a suitable value of p can be used. When dealing with text-based or sparse data, cosine similarity may be more appropriate. However, it is important to experiment with different distance metrics to find the one that works best for the given problem.\n",
    "\n",
    "\n",
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance?\n",
    "\n",
    "Ans:- following are some common hyperparameters in KNN:- n_neighbors, *, weights, algorithm, leaf_size, p, metric, metric_params, n_jobs\n",
    "\n",
    "they affect the performance of the model as model performance depends on no of neighbout, and we have to sellect best k value by hyper parameter tuning, similarly p value i.e. type of distance metric, which we have to find by hyperparamentr testing, we can go for grid searchh cv to find best optimum hyperparameters.\n",
    "\n",
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "techniques can be used to optimize the size of the training set?\n",
    "\n",
    "Ans:- As the number of dimensions increases, the amount of data required to cover the feature space increases exponentially. This means that the number of samples needed to cover the space becomes unmanageably large, and the algorithm may become too computationally expensive to use. Additionally, with high dimensional data, there may be a lot of noise and irrelevant features, which can negatively affect the performance of KNN.\n",
    "\n",
    "Therefore, when working with high-dimensional data, it is important to carefully consider the feature selection and data preprocessing methods to reduce the dimensionality of the data, and improve the performance of KNN.\n",
    "\n",
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "overcome these drawbacks to improve the performance of the model?\n",
    "\n",
    "Ans:- \n",
    "Weaknesses:\n",
    "\n",
    "Computationally expensive.\n",
    "\n",
    "Sensitive to the choice of distance metric and k.\n",
    "\n",
    "KNN may not perform well on imbalanced datasets.\n",
    "\n",
    "To address the weaknesses of KNN algorithm, the following strategies can be used:\n",
    "\n",
    "Feature selection.\n",
    "\n",
    "Experimenting with different distance metrics can help find the best one for the given problem.\n",
    "\n",
    "Preprocessing techniques such as feature scaling, outlier removal, and handling of missing values can improve the performance of KNN.\n",
    "\n",
    "Ensembling: Ensembling techniques such as bagging and boosting can be used to improve the performance of KNN by combining multiple models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
